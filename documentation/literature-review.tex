%!TEX ROOT = main.tex
\chapter{Literature Review}
\section{Introduction}
This study is investigating the effect of different document representation methods pair with different dimension reduction methods have on the performance of text classification models. The 3 aspects mentioned cover the whole pipeline of text classification, which are feature extraction from raw text, document representation; reduce the dimension of the vector space model, dimension reduction; and the classification models. The main focus on this study remain on the effect of dimension reduction has on text classification models.\\

\section{Document Representation}
\subsection{Term Frequency}
Term frequency is the simplest form of document representation method in BoW approach. It simply convert the words appeared in all the documents or text into a matrix or vector space model.

The columns are all the words appear in the document, each column represent one word. Each of the rows represent each of the document. The value in the cells represent the number of times a word appear in a document, in other word, the value is the term frequency.

Even though it is a simple document representation method, it is proven to be able to achieve a satisfactory results with the right processing and methods. \cite{knnVectorSpaceReduction}. In addition, term frequency would produce a big and sparse matrix which is exactly what this study is focusing on. Therefore, term frequency would be a suitable document representation algorithm to be applied in this study.\\

\clearpage
\subsection{Term Frequency - Inverse Document Freqeuncy (TF-IDF)}
Term frequency-Inverse document frequency (TF-IDF) is also a BoW approach. It is build on the concept of term frequency but take it a step further to overcome the setback of term frequency. 

Other than term frequency, TF-IDF also take into account the inverse document frequency which means that it also take the frequency of words in other documents into account. TF-IDF provide a measure of weight or importance to the words. The value of TF-IDF estimate the amount of information provided by each word in its document. The value of TF-IDF increases proportionally to the number of times a word appears in a document but is offset by the frequency of the word in the corpus. \cite{textMiningTfidf} The characteristic of TF-IDF can counter the high frequency of some common words, such as the stop words in a language. 

The main differences between term frequency and TF-IDF is that in term frequency, if a word has a high frequency over many documents it would be prominent features of the data. In TF-IDF however, term frequency alone would not make a word a prominent feature. If a word has a high frequency in many documents, the inverse document frequency of TF-IDF would offset its high frequency and it would not be a prominent feature. A word with high TF-IDF would appear in high frequency in a document but not many times in other documents. 

The vector space model resulting from TF-IDF would be in decimals as the values are assigned are scores or weights for each of the words. The resulting vector space model from term frequency on the other hand, consists of integers which represent the frequency of the words appear in the text.

TF-IDF is a simple document representation method that has proven to be efficient and reliable. It has a few setbacks as it only take into account the term frequency and inverse document frequency and not the semantic of the words. \cite{tfidfDrawback}. However, the focus area of this study is on the sparse and huge vector space model which TF-IDF produces.\\

Formula for TF-IDF is shown below:

\begin{equation}
TFIDF = tf \times \log_e \frac{N}{df}
\end{equation}
	
where:
	
\begin{center}
\begin{tabular}{l @{ $=$ } l}
	$tf$ & the number of times a word appears in a document \\
	$N$ & the total number of documents in the corpus \\
	$df$ & the number of documents that contain the word
\end{tabular}
\end{center}

\subsection{Summary}
Both term frequency and TF-IDF are chosen as the document representation method in this study. Term frequency and TF-IDF are simple and relatively easy to implement. Both of the methods are efficient and tried and true over the years. Most importantly, the output of huge and sparse matrix are the topic of study.\\

\clearpage
\section{Dimension Reduction}
\subsection{Principal Component Analysis (PCA)}
Principal component analysis is probably one of the most popular multivariate statistical technique in dimension reduction. PCA would output a transform the data from a data table with values from inter-related variables into new orthogonal variables. The variables would be the principal components of the data. In other words, PCA transform high dimensional space into low dimensional space by linear transformation while preserving the original data features as much as possible. \cite{pcaImage} The output from PCA is the principal components which are the important features in the data, the data that holds the most information.

PCA has several objectives, it would extract the most important information from the dataset, the dimension of the dataset would be compressed so that only the important information remained. The dataset is also simplified after PCA has processed it. \cite{pcaObj}

PCA has been successfully applied to image classification problems. After the features are extracted from the images, PCA is applied to extract the important features from the images. The reduction is dimension allow the researchers to save storage space for the images and still able to process the images in a satisfactory manner. The study also found that PCA is applied to reduce a high dimensional data, memory consumption and time taken would be high. \cite{pcaImage}

PCA is proven to be efficient in pattern recognition and image analysis and has been extensively employed for face recognition system. PCA also has been found that it is not capable of processing data with high dimensionality and sparsity. PCA is only effective when reducing tens or few hundreds of dimensions. \cite{dimRedCat}. Thus, PCA might not be suitable to be applied in this study which involves text data and the text data would be converted into a large and sparse matrix.\\

\subsection{Nonnegative Matrix Factorization (NMF)}
Nonnegative matrix factorization (NMF) is a multiplicative updating solution to decompose a nonnegative temporal-frequency data matrix into the sum of individual components. The sum of individual components are calculated from a nonnegative basis matrix. \cite{nmfBook}

NMF and its variant have found to be applied in many fields such as feature extractions, segmentation and clustering, dimension reduction and others. In PCA and SVD, the signs of the sign of the data is not restricted in any way but NMF has a non-negativity constraint. This means that NMF can only described by using additive components only. This is due to classical studied before, most of the values are in the positives. This non-negativity constraint of NMF has proved to be problematic when the data matrix is not strictly in the positives. \cite{semiNmfPca}

NMF has been applied to learn the latent space of the input text data from Twitter. With NMF the data is decomposed into bi-orthogonal non-negative 3-factor, the rows and columns from the different axis are simultaneously clustered. \cite{nmfTwitter}

NMF might be more suitabled for clustering as it took the least amount of time in clustering the data compared with SVD and PCA. \cite{nmfClustering}. This advantage of NMF over SVD and PCA is negligible in this study since this study would focus on classification rather than clustering.\\

\clearpage
\subsection{Truncated Single Value Decomposition (SVD)}
Single value decomposition is one of the most commonly used dimension reduction algorithms. It generalizes a complex matrix with many dimensions into a matrix of lower dimension via an extension of the polar decomposition. SVD detects the part of the data that contains the maximum variance in a set of orthogonal basis vectors. The data with the maximum variance would be the most prominent features of the data. \cite{svdDef}

The mathematical equation for singular value decomposition of a matrix X is shown as follows:
\begin{equation}
X = USV^{T}
\end{equation}

where:
\begin{center}
	\begin{tabular}{l @{ $=$ } l}
		$U$ & an $m \times n$ matrix, columns are left singular vectors \\
		$S$ & an $n \times n$ non-zero diagonal matrix, the singular values \\
		$V^{T}$ & an $n \times n$ matrix, rows are right singular vectors \\
	\end{tabular}
\end{center}

	
Latent Semantic Analysis (LSA) a technique applied in natural language processing that apply SVD in its process. SVD is applied in LSA to transform the features by dropping the least significant values in the matrix thus reducing the dimensions of the matrix. \cite{fuzzyLash}

Truncated SVD is a variant of SVD. Similar to PCA, truncated SVD is a matrix factorization technique that factors matrix M into 3 matrices namely, $U$, $\Sigma$ and $V$. There is a slight difference that differ truncated SVD from PCA, PCA performed the factorization on the covariance matrix while truncated SVD performed the factorization onto the data matrix directly. Truncated SVD differ slightly from SVD in the way of that SVD would always produce matrices of n columns if given $n \times n$ matrix but truncated SVD given the same matrix can produce matrices with specified number of columns. \cite{truncatedSVD}

Truncated SVD can reduced the dimension of the data into lesser dimension when compare to other dimension reduction algorithms such as LDA and PCA. The classification model trained with the output of SVD is also higher than that of LDA and PCA. \cite{dimRedCat}. Thus, SVD would be a great choice for the dimension reduction algorithm in this study.\\
	
\subsection{Summary}
The 3 dimension reduction algorithms above are considered to be blind source separation (BSS) methods, unsupervised learning algorithms. Out of the 3 dimension reduction algorithms, truncated SVD is the most suitable to be applied in this study. This is because PCA is not efficient in reducing high dimensional and sparse data and NMF are most efficient to be used in clustering algorithm. Most importantly, truncated SVD overcome the drawback of PCA and has been proven to be effective and achieve better result than PCA and others. Thus, truncated SVD is chosen to be the dimension reduction algorithm to be applied the feature matrix extracted from the text.\\

\clearpage	
\section{Document Classification}
\subsection{Support Vector Machine}
Support vector machine is a machine learning algorithm that construct a hyper plane to separate the examples into different classes. It has been proven to be very effective in dealing with high dimensional data. \cite{webSvm}. It is also proven to produce dramatically best results for topic modelling in experiments with the Reuters dataset. \cite{inductiveText}. Various issues need to be considered when applying SVM in document classification, the processing of the data, which kernel to use, and the parameters of SVM. A variant of SVM, called one-class SVM which is trained only with positive information has been used in document classification. \cite{oneSvm}.  The authors experimented with different kernels of SVM (linear, sigmoid, polynomial, and radial basis) with different type of document representation method (binary representation, frequency representation, TF-IDF representation, and Hadamard representation). The best result (F1 score of 0.507) is achieved with binary representation, feature length 10 and with linear kernel function.  
	
In another research, the researchers apply SVM in the classification on web document instead of news or ordinary text document. The document representation method used in this research is vector space model, just the nouns term in the web pages. The researchers experimented with different SVM kernels and varying the size of the training sets. Expectedly, the precision, recall and accuracy increased as the size of the training set increase. Linear kernel achieved the best result out of the various SVM kernels, a classification accuracy of 80\% is achieved. \cite{webSvm}.
	
SVM is relatively new compare to others algorithm in the field of document representation. It is not very efficient with large number of observations and it can be tricky to find an appropriate kernel for the problem.


\clearpage
\subsection{k-Nearest Neighbours (kNN)}
kNN is a classification machine learning algorithm that classify objects based on the closest training examples in the feature space based on a similarity measure. It is a simple and effective classification, as it only need 3 prerequisites. The 3 prerequisites are training dataset, similarity measure and the value of k which is the number of closest neighbours to be considered. 
	
kNN needs minimal training, it only needs to plot the training examples into a feature space.  
kNN has been applied in document classification before, it is found that kNN take significant 
longer time to classify a document into a topic. This is because kNN uses all the features of the data to compute the distance. Since the authors are using term vector space document 
representation method, the dimension of the feature space is high, thus the more time is needed for kNN to compute all the distance between the test object with the training objects. Other than the time taken to compute the distance, the k value is another obstacle in kNN algorithm. In a high dimensionality feature space and the points are not evenly distributed, the k value is hard to be determined.
	
To overcome the problems mentioned above, the authors applied term vector space reduction method, divide the document feature matrix into parts. Term vector space reduction reduces 
the sparsity of the document term matrix by removing the features less appeared in the corpus. By reducing the term vector space, a slight deterioration in the classification accuracy but the time cost is dramatically reduced. kNN still achieved an accuracy of 92.7\% but the time taken reduced from 53 minutes to 11 minutes. \cite{knnVectorSpaceReduction}

\clearpage
\subsection{Neural Network}
Neural network has a resurgence in recent years as there is a breakthrough in the neural network as Geofrey Hinton (et al.) discovered a technique called Contrastive Divergence that could quickly model inputs in a Restricted Boltzmann Machine (RBM). RBM is a 2-layer neural network that model the input by bouncing it through the network. This process is less computationally complex than backpropagation. \cite{nnHinton}.
	
Currently, neural network is applied in deep learning to solve various problems, document representation is one of them. Ranjan (et al.) applied Lion Fuzzy Neural Network on document 
classification. The researchers used WordNet ontology to retrieve the semantic of the words, 
and then added the context information onto it, thus the features obtained are semantic-context features. The classification part is performed by Lion Fuzzy Neural Network, which is a variant of Back Propagation Lion (BPLion) Neural Network that includes fuzzy bounding and Lion Algorithm. The neural network model used is trained incrementally. It achieves a higher accuracy than Naïve Bayes and some variant of the Lion Neural Network. \cite{lionNn}
	
Other than the modified neural network shown above, a simple feed-forward neural network is also efficient in document classification. By using the Hadamard product as document representation method, a simple neural network also can achieve a good classification accuracy in document classification compare to Naïve Bayes, kNN, and SVM. \cite{oneNn}
	
\section{Conclusion}
From the review of dimension reduction algorithms, PCA performed best compared to SVD and NMF. In addition, PCA performed well in text classification field. 
	
In machine learning algorithms for text classification, all 3 machine learning algorithms reviewed above would be applied. One of the objectives of this study is to investigate the performance of different machine learning algorithms in text classification. The same dataset would be used to train 3 models and the performance would be evaluated.
