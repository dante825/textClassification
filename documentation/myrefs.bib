% Encoding: UTF-8
@INPROCEEDINGS{knnVectorSpaceReduction, 
	author={A. {Moldagulova} and R. B. {Sulaiman}}, 
	booktitle={2018 18th International Conference on Control, Automation and Systems (ICCAS)}, 
	title={Document Classification Based on KNN Algorithm by Term Vector Space Reduction}, 
	year={2018}, 
	volume={}, 
	number={}, 
	pages={387-391}, 
	keywords={data analysis;data mining;information retrieval;nearest neighbour methods;pattern classification;text analysis;vectors;KNN algorithm;text mining;document classification methods;term-document matrix;data analysis;vector space model;information retrieval;knowledge discovery;unstructured textual data handling;vector space reduction;Classification algorithms;Text mining;Data models;Task analysis;Text categorization;Sparse matrices;Unstructured data;textual data;text mining;text classifier;document classification;term vector space reduction}, 
	doi={}, 
	ISSN={}, 
	month={Oct}
}

@inproceedings{inductiveText,
	author = {Dumais, Susan and Platt, John and Heckerman, David and Sahami, Mehran},
	title = {Inductive Learning Algorithms and Representations for Text Categorization},
	booktitle = {Proceedings of the Seventh International Conference on Information and Knowledge Management},
	series = {CIKM '98},
	year = {1998},
	isbn = {1-58113-061-9},
	location = {Bethesda, Maryland, USA},
	pages = {148--155},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/288627.288651},
	doi = {10.1145/288627.288651},
	acmid = {288651},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {classification, information management, machine learning, support vector machines, text categorization},
} 

@article{nnHinton,
	author = {Hinton, Geoffrey E.},
	title = {Training Products of Experts by Minimizing Contrastive Divergence},
	journal = {Neural Comput.},
	issue_date = {August 2002},
	volume = {14},
	number = {8},
	month = aug,
	year = {2002},
	issn = {0899-7667},
	pages = {1771--1800},
	numpages = {30},
	url = {http://dx.doi.org/10.1162/089976602760128018},
	doi = {10.1162/089976602760128018},
	acmid = {639730},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@article{oneSvm,
	author = {Manevitz, Larry M. and Yousef, Malik},
	title = {One-class Svms for Document Classification},
	journal = {J. Mach. Learn. Res.},
	issue_date = {3/1/2002},
	volume = {2},
	month = mar,
	year = {2002},
	issn = {1532-4435},
	pages = {139--154},
	numpages = {16},
	url = {http://dl.acm.org/citation.cfm?id=944790.944808},
	acmid = {944808},
	publisher = {JMLR.org},
} 

@article{oneNn,
	title = "One-class document classification via Neural Networks",
	journal = "Neurocomputing",
	volume = "70",
	number = "7",
	pages = "1466 - 1481",
	year = "2007",
	note = "Advances in Computational Intelligence and Learning",
	issn = "0925-2312",
	doi = "https://doi.org/10.1016/j.neucom.2006.05.013",
	url = "http://www.sciencedirect.com/science/article/pii/S092523120600261X",
	author = "Larry Manevitz and Malik Yousef",
	keywords = "Classification, Automated document retrieval, Feed-forward neural networks, Machine learning, One-class classification, Autoencoder, Bottleneck neural network",
	abstract = "Automated document retrieval and classification is of central importance in many contexts; our main motivating goal is the efficient classification and retrieval of “interests” on the internet when only positive information is available. In this paper, we show how a simple feed-forward neural network can be trained to filter documents under these conditions, and that this method seems to be superior to modified methods (modified to use only positive examples), such as Rocchio, Nearest Neighbor, Naive-Bayes, Distance-based Probability and One-Class SVM algorithms. A novel experimental finding is that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation (“Hadamard”) of the information prior to the training of the network."
}

@article{lionNn,
	title = "LFNN: Lion fuzzy neural network-based evolutionary model for text classification using context and sense based features",
	journal = "Applied Soft Computing",
	volume = "71",
	pages = "994 - 1008",
	year = "2018",
	issn = "1568-4946",
	doi = "https://doi.org/10.1016/j.asoc.2018.07.016",
	url = "http://www.sciencedirect.com/science/article/pii/S1568494618304046",
	author = "Nihar M. Ranjan and Rajesh S. Prasad",
	keywords = "Text classification, Connectionist model, Incremental learning, Semantic word processing, Contextual word computation, Neural network, Fuzzy bounding",
	abstract = "Text classification is one of the popular techniques of text mining that labels the documents based on a set of topics defined according to the requirements. Among various approaches used for text categorization, incremental learning techniques are important due to its widespread applications. This paper presents a connectionist classification approach using context-semantic features and LFNN-based incremental learning algorithm for the text classification. The proposed technique considers a dynamic database for the classification so that the classifier can learn the model dynamically. This incremental learning process adopts Back Propagation Lion (BPLion) Neural Network, where it includes fuzzy bounding and Lion Algorithm (LA), for the feasible selection of weights. The effectiveness of the proposed method is analyzed by comparing it with the existing techniques, I-BP, FI-BP, and I-BPLion regarding accuracy and error, in a comparative analysis. As a result of the comparison, classification accuracies of 81.49%, 83.41%, 88.76%, and 95%; and minimum error values of 8.11, 7.49, 3.02, and 4.92 are possible to attain in LFNN, for 20 Newsgroup, Reuter datasets, WebKB, and RCV1 respectively."
}

@InProceedings{webSvm,
  author    = {Shinde, Sharmila and Joeg, Prasanna and Vanjale, Sandeep},
  title     = {Web Document Classification using Support Vector Machine},
  booktitle = {2017 International Conference on Current Trends in Computer, Electrical, Electronics and Communication (CTCEEC)},
  year      = {2017},
  pages     = {688-691},
  month     = {09},
  doi       = {10.1109/CTCEEC.2017.8455102},
}

@article{vectorSpaceModelText,
	author = {Ababneh, Jafar and Almanmomani, Omar and Hadi, Wael and El-Omari, Nidhal and Alibrahim, Ali},
	year = {2014},
	month = {02},
	pages = {219-223},
	title = {Vector Space Models to Classify Arabic Text},
	volume = {7},
	journal = {International Journal of Computer Trends and Technology (IJCTT},
	doi = {10.14445/22312803/IJCTT-V7P109}
}

@article{tfidf,
	title = {Unsupervised sentence representations as word information series: Revisiting TF–IDF},
	journal = {Computer Speech and Language},
	volume = {56},
	pages = {107-129},
	year = {2019},
	issn = {0885-2308},
	doi = {https://doi.org/10.1016/j.csl.2019.01.005},
	url = {http://www.sciencedirect.com/science/article/pii/S0885230817302887},
	author = {Ignacio Arroyo-Fernández and Carlos-Francisco Méndez-Cruz and Gerardo Sierra and Juan-Manuel Torres-Moreno and Grigori Sidorov},
	keywords = {Sentence representation, Sentence embedding, Word embedding, Information entropy, TF–IDF, Natural language processing}
}

@article{pcaImage,
	title = "Dimension reduction of image deep feature using PCA",
	journal = "Journal of Visual Communication and Image Representation",
	volume = "63",
	pages = "102578",
	year = "2019",
	issn = "1047-3203",
	doi = "https://doi.org/10.1016/j.jvcir.2019.102578",
	url = "http://www.sciencedirect.com/science/article/pii/S1047320319301932",
	author = "Ji Ma and Yuyu Yuan",
	keywords = "Deep learning, Feature extraction, Dimension reduction, PCA algorithm",
	abstract = "Convolution neural networks based methods can derive deep features from training images. However, one challenge is that the dimension of the extracted image features increases dramatically with more network layers. To solve this problem, this paper focuses on the study of dimension reduction. After using deep learning to extract image features, the PCA algorithm is used to achieve dimension reduction. Specifically, we first leverage deep convolutional neural network to extract image features. Then, we introduce and leverage PCA algorithm to achieve dimension reduction. Aiming at the problem that it is difficult to process high-dimensional sparse big data based on PCA algorithm. This paper optimizes the PCA algorithm. After image preprocessing, the feasibility of PCA algorithm for dimension reduction of image feature extraction by deep learning is verified by simulation experiments. The efficiency of the proposed algorithm is proved by comparing the performance of PCA algorithm before and after optimization."
}

@INPROCEEDINGS{lingo, 
	author={S. A. {Narhari} and R. {Shedge}}, 
	booktitle={2017 International Conference on Advances in Computing, Communication and Control (ICAC3)}, 
	title={Text categorization of Marathi documents using modified LINGO}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={1-5}, 
	keywords={information retrieval;natural language processing;pattern classification;pattern clustering;principal component analysis;query processing;search engines;singular value decomposition;text analysis;search engine;clustering technique;classification technique;label induction grouping;single value decomposition;SVD;dimension reduction;principle component analysis;PCA;automatic text categorization;regional language;modified LINGO algorithm;Marathi text documents;Text categorization;Clustering algorithms;Feature extraction;Principal component analysis;Supervised learning;Dimensionality reduction;Text mining;Text Categorization;Dimension reduction;Morphological analysis LINGO;SVD;PCA}, 
	doi={10.1109/ICAC3.2017.8318771}, 
	ISSN={}, 
	month={Dec},}

@article{dimReducArabic,
	title = "An effective dimension reduction algorithm for clustering Arabic text",
	journal = "Egyptian Informatics Journal",
	year = "2019",
	issn = "1110-8665",
	doi = "https://doi.org/10.1016/j.eij.2019.05.002",
	url = "http://www.sciencedirect.com/science/article/pii/S1110866518301579",
	author = "A.A. Mohamed",
	keywords = "Clustering, Dimensionality reduction, PCA, SVD, NMF, Arabic NLP",
	abstract = "Text clustering is a challenging task in natural language processing due to the very high dimensional space produced by this process (i.e. curse of dimensionality problem). Since these texts contain considerable amounts of ambiguities and redundancies, they produce different noise effects. For an efficient and accurate clustering algorithm, we need to extract the main concepts of the text by eliminating the noise and reducing the high dimensionality of the data. This paper compares among three of the famous dimension reduction algorithms for text clustering to show the pros and cons of each one, namely Principal Component Analysis (PCA), Nonnegative Matrix Factorization (NMF) and Singular Value Decomposition (SVD). It presents an effective dimension reduction algorithm for Arabic text clustering using PCA. For that purpose, a series of the experiments has been conducted using two linguistic corpora for both English and Arabic and analyzed the results from a clustering quality point of view. The experiments have shown that PCA improves the quality of the clustering process and that it gives more interpretable results with less time needed for the clustering process for both Arabic and English documents."
}

@INPROCEEDINGS{treePca, 
	author={Y. {Su} and Y. {Huang} and C. -. J. {Kuo}}, 
	booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
	title={Efficient Text Classification Using Tree-structured Multi-linear Principal Component Analysis}, 
	year={2018}, 
	volume={}, 
	number={}, 
	pages={585-590}, 
	keywords={pattern classification;principal component analysis;recurrent neural nets;support vector machines;text analysis;trees (mathematics);tree-structured multilinear principal component analysis;principal component analysis;text data dimension reduction;TMPCA;text classification;PCA;support vector machine;SVM;recurrent neural network;RNN;Principal component analysis;Support vector machines;Training;Dimensionality reduction;Task analysis;Complexity theory;Tools}, 
	doi={10.1109/ICPR.2018.8545832}, 
	ISSN={1051-4651}, 
	month={Aug},}

@article{svdDef,
	author = {Sweeney, Elizabeth and Vogelstein, Joshua and Cuzzocreo, Jennifer and A Calabresi, Peter and Reich, Daniel and M Crainiceanu, Ciprian and T Shinohara, Russell},
	year = {2014},
	month = {04},
	pages = {e95753},
	title = {A Comparison of Supervised Machine Learning Algorithms and Feature Vectors for MS Lesion Segmentation Using Multimodal Structural MRI},
	volume = {9},
	journal = {PloS one},
	doi = {10.1371/journal.pone.0095753}
}

@INPROCEEDINGS{fuzzyLash, 
	author={A. {Karami}}, 
	booktitle={2017 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
	title={Taming Wild High Dimensional Text Data with a Fuzzy Lash}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={518-522}, 
	keywords={feature extraction;fuzzy set theory;pattern clustering;principal component analysis;singular value decomposition;text analysis;vectors;wild high dimensional text data;fuzzy lash;high-dimensional sparse vector;dimension reduction;DR method;Unsupervised Feature Transformation;UFT strategy;fuzzy clustering;BOW matrix;bag of words;Principal Components Analysis;PCA;Singular Value Decomposition;SVD;Principal component analysis;Matrix converters;Sparse matrices;Feature extraction;Data mining;Clustering algorithms;dimension reduction;fuzzy clustering;SVD;PCA;classification}, 
	doi={10.1109/ICDMW.2017.73}, 
	ISSN={2375-9259}, 
	month={Nov},}

@incollection{nmfBook,
	title = "Chapter 5 - Nonnegative Matrix Factorization",
	editor = "Jen-Tzung Chien",
	booktitle = "Source Separation and Machine Learning",
	publisher = "Academic Press",
	pages = "161 - 229",
	year = "2019",
	isbn = "978-0-12-817796-9",
	doi = "https://doi.org/10.1016/B978-0-12-804566-4.00017-6",
	url = "http://www.sciencedirect.com/science/article/pii/B9780128045664000176",
	author = "Jen-Tzung Chien",
	keywords = "signal-channel source separation, probabilistic nonnegative factorization, variational Bayesian, sampling method, Bayesian learning, sparse learning, discriminative learning, deep learning, speech dereverberation, speech separation, music signal separation, singing voice separation, music information retrieval"
}

@ARTICLE{semiNmfPca, 
	author={K. {Allab} and L. {Labiod} and M. {Nadif}}, 
	journal={IEEE Transactions on Knowledge and Data Engineering}, 
	title={A Semi-NMF-PCA Unified Framework for Data Clustering}, 
	year={2017}, 
	volume={29}, 
	number={1}, 
	pages={2-16}, 
	keywords={data reduction;matrix decomposition;pattern clustering;principal component analysis;continuous dimension reduction;data reduction;data clustering;PCA;principal component analysis;NMF;nonnegative matrix factorization;Principal component analysis;Clustering algorithms;Linear programming;Partitioning algorithms;Optimization;Matrix decomposition;Clustering methods;Clustering;dimension reduction;locality preserving}, 
	doi={10.1109/TKDE.2016.2606098}, 
	ISSN={1041-4347}, 
	month={Jan},}


@article{dimRedCat,
	title={Unified Framework of Dimensionality Reduction and Text Categorisation},
	author={Rajashekharaiah, KMM and Chikkalli, Sunil S and Kumbar, Prateek K and Babu, P Suryanarayana},
	journal={International Journal of Engineering \& Technology},
	volume={7},
	number={3.29},
	pages={648--654},
	year={2018}
}

@article{textMiningTfidf,
	title={Preprocessing techniques for text mining-an overview},
	author={Vijayarani, S and Ilamathi, Ms J and Nithya, Ms},
	journal={International Journal of Computer Science \& Communication Networks},
	volume={5},
	number={1},
	pages={7--16},
	year={2015}
}

@inproceedings{stemLemma,
	title = "Leveraging Inflection Tables for Stemming and Lemmatization.",
	author = "Nicolai, Garrett  and
	Kondrak, Grzegorz",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P16-1108",
	doi = "10.18653/v1/P16-1108",
	pages = "1138--1147",
}

@INPROCEEDINGS{bigData,
	author={M. U. {Çakir} and S. {Güldamlasioğlu}},
	booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)},
	title={Text Mining Analysis in Turkish Language Using Big Data Tools},
	year={2016},
	volume={1},
	number={},
	pages={614-618},
	keywords={Big Data;data analysis;data mining;feature selection;natural language processing;pattern clustering;social networking (online);text analysis;text mining analysis;Turkish language;Big Data infrastructure;social media;feature selection;topic clustering;Sparks;Text mining;Media;Big data;Data models;Feature extraction;Clustering algorithms;Text mining;Text preprocessing;Topic modeling;Text clustering;Big data},
	doi={10.1109/COMPSAC.2016.203},
	ISSN={},
	month={June},}

@article{mittal2019performance,
	title={Performance study of K-nearest neighbor classifier and K-means clustering for predicting the diagnostic accuracy},
	author={Mittal, Kavita and Aggarwal, Gaurav and Mahajan, Prerna},
	journal={International Journal of Information Technology},
	volume={11},
	number={3},
	pages={535--540},
	year={2019},
	publisher={Springer}
}



@COMMENT
End here

@INPROCEEDINGS{8524416, 
author={G. N. {Chandrika} and E. S. {Reddy}}, 
booktitle={2017 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)}, 
title={An Efficient Filtered Classifier for Classification of Unseen Test Data in Text Documents}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-4}, 
keywords={data mining;decision trees;learning (artificial intelligence);pattern classification;text analysis;text documents;testing data sets;text document data;C4.5 decision tree classifier;unseen test documents;classifier accuracy;unseen test data;automatic classification;e-documents;data repositories;unseen data;document data distribution;electronic documents;Text categorization;Filtering algorithms;Classification algorithms;Twitter;Training;Testing;Feature extraction;filtered classifier;text documents;discretization}, 
doi={10.1109/ICCIC.2017.8524416}, 
ISSN={2473-943X}, 
month={Dec},}

@article{CHEN20191,
title = "Experimental explorations on short text topic mining between LDA and NMF based Schemes",
journal = "Knowledge-Based Systems",
volume = "163",
pages = "1 - 13",
year = "2019",
issn = "0950-7051",
doi = "https://doi.org/10.1016/j.knosys.2018.08.011",
url = "http://www.sciencedirect.com/science/article/pii/S0950705118304076",
author = "Yong Chen and Hui Zhang and Rui Liu and Zhiwen Ye and Jianying Lin",
keywords = "Short text mining, Topic modeling, Latent dirichlet allocation (LDA), Non-negative matrix factorization (NMF), Knowledge-based learning",
abstract = "Learning topics from short texts has become a critical and fundamental task for understanding the widely-spread streaming social messages, e.g., tweets, snippets and questions/answers. Up to date, there are two distinctive topic learning schemes: generative probabilistic graphical models and geometrically linear algebra approaches, with LDA and NMF being the representative works, respectively. Since these two methods both could uncover the latent topics hidden in the unstructured short texts, some interesting doubts are coming to our minds that which one is better and why? Are there any other more effective extensions? In order to explore valuable insights between LDA and NMF based learning schemes, we comprehensively conduct a series of experiments into two parts. Specifically, the basic LDA and NMF are compared with different experimental settings on several public short text datasets in the first part which would exhibit that NMF tends to perform better than LDA; in the second part, we propose a novel model called “Knowledge-guided Non-negative Matrix Factorization for Better Short Text Topic Mining” (abbreviated as KGNMF), which leverages external knowledge as a semantic regulator with low-rank formalizations, yielding up a time-efficient algorithm. Extensive experiments are conducted on three representative corpora with currently typical short text topic models to demonstrate the effectiveness of our proposed KGNMF. Overall, learning with NMF-based schemes is another effective manner in short text topic mining in addition to the popular LDA-based paradigms."
}

@article{Elberrichi2008UsingWF,
  title={Using WordNet for Text Categorization},
  author={Zakaria Elberrichi and Abdellatif Rahmoun and Mohamed Amine Bentaallah},
  journal={Int. Arab J. Inf. Technol.},
  year={2008},
  volume={5},
  pages={16-24}
}

@article{article,
author = {Harish, B S and Guru, Devanur and Shantharamu, Manjunath},
year = {2010},
month = {01},
pages = {110 - 119},
title = {Representation and Classification of Text Documents: A Brief Review},
volume = {1},
journal = {International Journal of Computer Applications,Special Issue on RTIPPR}
}

@article{KIM201915,
title = "Multi-co-training for document classification using various document representations: TF–IDF, LDA, and Doc2Vec",
journal = "Information Sciences",
volume = "477",
pages = "15 - 29",
year = "2019",
issn = "0020-0255",
doi = "https://doi.org/10.1016/j.ins.2018.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S0020025518308028",
author = "Donghwa Kim and Deokseong Seo and Suhyoun Cho and Pilsung Kang",
keywords = "Document classification, Semi-supervised learning, TF–IDF, LDA, Doc2vec, Co-training",
abstract = "The purpose of document classification is to assign the most appropriate label to a specified document. The main challenges in document classification are insufficient label information and unstructured sparse format. A semi-supervised learning (SSL) approach could be an effective solution to the former problem, whereas the consideration of multiple document representation schemes can resolve the latter problem. Co-training is a popular SSL method that attempts to exploit various perspectives in terms of feature subsets for the same example. In this paper, we propose multi-co-training (MCT) for improving the performance of document classification. In order to increase the variety of feature sets for classification, we transform a document using three document representation methods: term frequency–inverse document frequency (TF–IDF) based on the bag-of-words scheme, topic distribution based on latent Dirichlet allocation (LDA), and neural-network-based document embedding known as document to vector (Doc2Vec). The experimental results demonstrate that the proposed MCT is robust to parameter changes and outperforms benchmark methods under various conditions."
}

@INPROCEEDINGS{7844892, 
author={H. {Kim} and J. {Kim} and {Jinseog Kim}}, 
booktitle={2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={Semantic text classification with tensor space model-based naïve Bayes}, 
year={2016}, 
volume={}, 
number={}, 
pages={004206-004210}, 
keywords={Bayes methods;pattern classification;semantic Web;tensors;text analysis;Web sites;semantic text classification;tensor space model-based naïve Bayes method;semantic naïve Bayes classification technique;text representation;Wikipedia articles;2nd-order tensor;term feature statistics;document collections;F1-measures;Reuters-21578 document collection;20Newsgroups document collection;Semantics;Tensile stress;Text categorization;Niobium;Classification algorithms;Encyclopedias;Training;text classification;naïve Bayes;vector space;tensor space;Wikipedia;semantics;concepts}, 
doi={10.1109/SMC.2016.7844892}, 
ISSN={}, 
month={Oct},}

@InProceedings{inproceedings,
  author    = {Hlaing Moe, Zun and San, Thida and Mie Khin, Mie and May Tin, Hlaing},
  title     = {Comparison Of Naive Bayes And Support Vector Machine Classifiers On Document Classification},
  booktitle = {Multitopic Conference (INMIC), 2011 IEEE 14th International Multitopic Conference},
  year      = {2018},
  pages     = {466-467},
  month     = {10},
  doi       = {10.1109/GCCE.2018.8574785},
}

@unknown{unknown,
author = {Sarkar, Koushiki},
year = {2015},
month = {09},
pages = {},
title = {A Novel Approach to Document Classification using WordNet},
doi = {10.13140/RG.2.1.2550.3200}
}

@Article{Ting2011,
  author  = {Ting, S.L. and Ip, W.H. and Tsang, Albert},
  title   = {Is Naïve Bayes a Good Classifier for Document Classification?},
  journal = {International Journal of Software Engineering and its Applications},
  year    = {2011},
  volume  = {5},
  month   = {01},
}

@Comment{jabref-meta: databaseType:bibtex;}

@COMMENT
Dimension Reduction references
