% Encoding: UTF-8
@INPROCEEDINGS{knnVectorSpaceReduction, 
	author={A. {Moldagulova} and R. B. {Sulaiman}}, 
	booktitle={2018 18th International Conference on Control, Automation and Systems (ICCAS)}, 
	title={Document Classification Based on KNN Algorithm by Term Vector Space Reduction}, 
	year={2018}, 
	volume={}, 
	number={}, 
	pages={387-391}, 
	keywords={data analysis;data mining;information retrieval;nearest neighbour methods;pattern classification;text analysis;vectors;KNN algorithm;text mining;document classification methods;term-document matrix;data analysis;vector space model;information retrieval;knowledge discovery;unstructured textual data handling;vector space reduction;Classification algorithms;Text mining;Data models;Task analysis;Text categorization;Sparse matrices;Unstructured data;textual data;text mining;text classifier;document classification;term vector space reduction}, 
	doi={}, 
	ISSN={}, 
	month={Oct}
}

@inproceedings{inductiveText,
	author = {Dumais, Susan and Platt, John and Heckerman, David and Sahami, Mehran},
	title = {Inductive Learning Algorithms and Representations for Text Categorization},
	booktitle = {Proceedings of the Seventh International Conference on Information and Knowledge Management},
	series = {CIKM '98},
	year = {1998},
	isbn = {1-58113-061-9},
	location = {Bethesda, Maryland, USA},
	pages = {148--155},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/288627.288651},
	doi = {10.1145/288627.288651},
	acmid = {288651},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {classification, information management, machine learning, support vector machines, text categorization},
} 

@article{nnHinton,
	author = {Hinton, Geoffrey E.},
	title = {Training Products of Experts by Minimizing Contrastive Divergence},
	journal = {Neural Comput.},
	issue_date = {August 2002},
	volume = {14},
	number = {8},
	month = aug,
	year = {2002},
	issn = {0899-7667},
	pages = {1771--1800},
	numpages = {30},
	url = {http://dx.doi.org/10.1162/089976602760128018},
	doi = {10.1162/089976602760128018},
	acmid = {639730},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@article{oneSvm,
	author = {Manevitz, Larry M. and Yousef, Malik},
	title = {One-class Svms for Document Classification},
	journal = {J. Mach. Learn. Res.},
	issue_date = {3/1/2002},
	volume = {2},
	month = mar,
	year = {2002},
	issn = {1532-4435},
	pages = {139--154},
	numpages = {16},
	url = {http://dl.acm.org/citation.cfm?id=944790.944808},
	acmid = {944808},
	publisher = {JMLR.org},
} 

@article{oneNn,
	title = "One-class document classification via Neural Networks",
	journal = "Neurocomputing",
	volume = "70",
	number = "7",
	pages = "1466 - 1481",
	year = "2007",
	note = "Advances in Computational Intelligence and Learning",
	issn = "0925-2312",
	doi = "https://doi.org/10.1016/j.neucom.2006.05.013",
	url = "http://www.sciencedirect.com/science/article/pii/S092523120600261X",
	author = "Larry Manevitz and Malik Yousef",
	keywords = "Classification, Automated document retrieval, Feed-forward neural networks, Machine learning, One-class classification, Autoencoder, Bottleneck neural network",
	abstract = "Automated document retrieval and classification is of central importance in many contexts; our main motivating goal is the efficient classification and retrieval of “interests” on the internet when only positive information is available. In this paper, we show how a simple feed-forward neural network can be trained to filter documents under these conditions, and that this method seems to be superior to modified methods (modified to use only positive examples), such as Rocchio, Nearest Neighbor, Naive-Bayes, Distance-based Probability and One-Class SVM algorithms. A novel experimental finding is that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation (“Hadamard”) of the information prior to the training of the network."
}

@article{lionNn,
	title = "LFNN: Lion fuzzy neural network-based evolutionary model for text classification using context and sense based features",
	journal = "Applied Soft Computing",
	volume = "71",
	pages = "994 - 1008",
	year = "2018",
	issn = "1568-4946",
	doi = "https://doi.org/10.1016/j.asoc.2018.07.016",
	url = "http://www.sciencedirect.com/science/article/pii/S1568494618304046",
	author = "Nihar M. Ranjan and Rajesh S. Prasad",
	keywords = "Text classification, Connectionist model, Incremental learning, Semantic word processing, Contextual word computation, Neural network, Fuzzy bounding",
	abstract = "Text classification is one of the popular techniques of text mining that labels the documents based on a set of topics defined according to the requirements. Among various approaches used for text categorization, incremental learning techniques are important due to its widespread applications. This paper presents a connectionist classification approach using context-semantic features and LFNN-based incremental learning algorithm for the text classification. The proposed technique considers a dynamic database for the classification so that the classifier can learn the model dynamically. This incremental learning process adopts Back Propagation Lion (BPLion) Neural Network, where it includes fuzzy bounding and Lion Algorithm (LA), for the feasible selection of weights. The effectiveness of the proposed method is analyzed by comparing it with the existing techniques, I-BP, FI-BP, and I-BPLion regarding accuracy and error, in a comparative analysis. As a result of the comparison, classification accuracies of 81.49%, 83.41%, 88.76%, and 95%; and minimum error values of 8.11, 7.49, 3.02, and 4.92 are possible to attain in LFNN, for 20 Newsgroup, Reuter datasets, WebKB, and RCV1 respectively."
}

@InProceedings{webSvm,
  author    = {Shinde, Sharmila and Joeg, Prasanna and Vanjale, Sandeep},
  title     = {Web Document Classification using Support Vector Machine},
  booktitle = {2017 International Conference on Current Trends in Computer, Electrical, Electronics and Communication (CTCEEC)},
  year      = {2017},
  pages     = {688-691},
  month     = {09},
  doi       = {10.1109/CTCEEC.2017.8455102},
}

@article{vectorSpaceModelText,
	author = {Ababneh, Jafar and Almanmomani, Omar and Hadi, Wael and El-Omari, Nidhal and Alibrahim, Ali},
	year = {2014},
	month = {02},
	pages = {219-223},
	title = {Vector Space Models to Classify Arabic Text},
	volume = {7},
	journal = {International Journal of Computer Trends and Technology (IJCTT},
	doi = {10.14445/22312803/IJCTT-V7P109}
}

@article{tfidf,
	title = {Unsupervised sentence representations as word information series: Revisiting TF–IDF},
	journal = {Computer Speech and Language},
	volume = {56},
	pages = {107-129},
	year = {2019},
	issn = {0885-2308},
	doi = {https://doi.org/10.1016/j.csl.2019.01.005},
	url = {http://www.sciencedirect.com/science/article/pii/S0885230817302887},
	author = {Ignacio Arroyo-Fernández and Carlos-Francisco Méndez-Cruz and Gerardo Sierra and Juan-Manuel Torres-Moreno and Grigori Sidorov},
	keywords = {Sentence representation, Sentence embedding, Word embedding, Information entropy, TF–IDF, Natural language processing}
}

@article{pcaImage,
	title = "Dimension reduction of image deep feature using PCA",
	journal = "Journal of Visual Communication and Image Representation",
	volume = "63",
	pages = "102578",
	year = "2019",
	issn = "1047-3203",
	doi = "https://doi.org/10.1016/j.jvcir.2019.102578",
	url = "http://www.sciencedirect.com/science/article/pii/S1047320319301932",
	author = "Ji Ma and Yuyu Yuan",
	keywords = "Deep learning, Feature extraction, Dimension reduction, PCA algorithm",
	abstract = "Convolution neural networks based methods can derive deep features from training images. However, one challenge is that the dimension of the extracted image features increases dramatically with more network layers. To solve this problem, this paper focuses on the study of dimension reduction. After using deep learning to extract image features, the PCA algorithm is used to achieve dimension reduction. Specifically, we first leverage deep convolutional neural network to extract image features. Then, we introduce and leverage PCA algorithm to achieve dimension reduction. Aiming at the problem that it is difficult to process high-dimensional sparse big data based on PCA algorithm. This paper optimizes the PCA algorithm. After image preprocessing, the feasibility of PCA algorithm for dimension reduction of image feature extraction by deep learning is verified by simulation experiments. The efficiency of the proposed algorithm is proved by comparing the performance of PCA algorithm before and after optimization."
}

@INPROCEEDINGS{lingo, 
	author={S. A. {Narhari} and R. {Shedge}}, 
	booktitle={2017 International Conference on Advances in Computing, Communication and Control (ICAC3)}, 
	title={Text categorization of Marathi documents using modified LINGO}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={1-5}, 
	keywords={information retrieval;natural language processing;pattern classification;pattern clustering;principal component analysis;query processing;search engines;singular value decomposition;text analysis;search engine;clustering technique;classification technique;label induction grouping;single value decomposition;SVD;dimension reduction;principle component analysis;PCA;automatic text categorization;regional language;modified LINGO algorithm;Marathi text documents;Text categorization;Clustering algorithms;Feature extraction;Principal component analysis;Supervised learning;Dimensionality reduction;Text mining;Text Categorization;Dimension reduction;Morphological analysis LINGO;SVD;PCA}, 
	doi={10.1109/ICAC3.2017.8318771}, 
	ISSN={}, 
	month={Dec},}

@article{dimReducArabic,
	title = "An effective dimension reduction algorithm for clustering Arabic text",
	journal = "Egyptian Informatics Journal",
	year = "2019",
	issn = "1110-8665",
	doi = "https://doi.org/10.1016/j.eij.2019.05.002",
	url = "http://www.sciencedirect.com/science/article/pii/S1110866518301579",
	author = "A.A. Mohamed",
	keywords = "Clustering, Dimensionality reduction, PCA, SVD, NMF, Arabic NLP",
	abstract = "Text clustering is a challenging task in natural language processing due to the very high dimensional space produced by this process (i.e. curse of dimensionality problem). Since these texts contain considerable amounts of ambiguities and redundancies, they produce different noise effects. For an efficient and accurate clustering algorithm, we need to extract the main concepts of the text by eliminating the noise and reducing the high dimensionality of the data. This paper compares among three of the famous dimension reduction algorithms for text clustering to show the pros and cons of each one, namely Principal Component Analysis (PCA), Nonnegative Matrix Factorization (NMF) and Singular Value Decomposition (SVD). It presents an effective dimension reduction algorithm for Arabic text clustering using PCA. For that purpose, a series of the experiments has been conducted using two linguistic corpora for both English and Arabic and analyzed the results from a clustering quality point of view. The experiments have shown that PCA improves the quality of the clustering process and that it gives more interpretable results with less time needed for the clustering process for both Arabic and English documents."
}

@INPROCEEDINGS{treePca, 
	author={Y. {Su} and Y. {Huang} and C. -. J. {Kuo}}, 
	booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, 
	title={Efficient Text Classification Using Tree-structured Multi-linear Principal Component Analysis}, 
	year={2018}, 
	volume={}, 
	number={}, 
	pages={585-590}, 
	keywords={pattern classification;principal component analysis;recurrent neural nets;support vector machines;text analysis;trees (mathematics);tree-structured multilinear principal component analysis;principal component analysis;text data dimension reduction;TMPCA;text classification;PCA;support vector machine;SVM;recurrent neural network;RNN;Principal component analysis;Support vector machines;Training;Dimensionality reduction;Task analysis;Complexity theory;Tools}, 
	doi={10.1109/ICPR.2018.8545832}, 
	ISSN={1051-4651}, 
	month={Aug},}

@article{svdDef,
	author = {Sweeney, Elizabeth and Vogelstein, Joshua and Cuzzocreo, Jennifer and A Calabresi, Peter and Reich, Daniel and M Crainiceanu, Ciprian and T Shinohara, Russell},
	year = {2014},
	month = {04},
	pages = {e95753},
	title = {A Comparison of Supervised Machine Learning Algorithms and Feature Vectors for MS Lesion Segmentation Using Multimodal Structural MRI},
	volume = {9},
	journal = {PloS one},
	doi = {10.1371/journal.pone.0095753}
}

@INPROCEEDINGS{fuzzyLash, 
	author={A. {Karami}}, 
	booktitle={2017 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
	title={Taming Wild High Dimensional Text Data with a Fuzzy Lash}, 
	year={2017}, 
	volume={}, 
	number={}, 
	pages={518-522}, 
	keywords={feature extraction;fuzzy set theory;pattern clustering;principal component analysis;singular value decomposition;text analysis;vectors;wild high dimensional text data;fuzzy lash;high-dimensional sparse vector;dimension reduction;DR method;Unsupervised Feature Transformation;UFT strategy;fuzzy clustering;BOW matrix;bag of words;Principal Components Analysis;PCA;Singular Value Decomposition;SVD;Principal component analysis;Matrix converters;Sparse matrices;Feature extraction;Data mining;Clustering algorithms;dimension reduction;fuzzy clustering;SVD;PCA;classification}, 
	doi={10.1109/ICDMW.2017.73}, 
	ISSN={2375-9259}, 
	month={Nov},}

@ARTICLE{semiNmfPca, 
	author={K. {Allab} and L. {Labiod} and M. {Nadif}}, 
	journal={IEEE Transactions on Knowledge and Data Engineering}, 
	title={A Semi-NMF-PCA Unified Framework for Data Clustering}, 
	year={2017}, 
	volume={29}, 
	number={1}, 
	pages={2-16}, 
	keywords={data reduction;matrix decomposition;pattern clustering;principal component analysis;continuous dimension reduction;data reduction;data clustering;PCA;principal component analysis;NMF;nonnegative matrix factorization;Principal component analysis;Clustering algorithms;Linear programming;Partitioning algorithms;Optimization;Matrix decomposition;Clustering methods;Clustering;dimension reduction;locality preserving}, 
	doi={10.1109/TKDE.2016.2606098}, 
	ISSN={1041-4347}, 
	month={Jan},}


@article{dimRedCat,
	title={Unified Framework of Dimensionality Reduction and Text Categorisation},
	author={Rajashekharaiah, KMM and Chikkalli, Sunil S and Kumbar, Prateek K and Babu, P Suryanarayana},
	journal={International Journal of Engineering \& Technology},
	volume={7},
	number={3.29},
	pages={648-654},
	year={2018}
}

@article{textMiningTfidf,
	title={Preprocessing techniques for text mining-an overview},
	author={Vijayarani, S and Ilamathi, Ms J and Nithya, Ms},
	journal={International Journal of Computer Science \& Communication Networks},
	volume={5},
	number={1},
	pages={7--16},
	year={2015}
}

@incollection{nmfClustering,
	title={Nonnegative matrix factorizations for clustering: A survey},
	author={Li, Tao and Ding, Cha-charis},
	booktitle={Data Clustering},
	pages={149--176},
	year={2018},
	publisher={Chapman and Hall/CRC}
}

@inproceedings{nmfTwitter,
	title={Joint non-negative matrix factorization for learning ideological leaning on twitter},
	author={Lahoti, Preethi and Garimella, Kiran and Gionis, Aristides},
	booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
	pages={351--359},
	year={2018},
	organization={ACM}
}

@inproceedings{stemLemma,
	title = {Leveraging Inflection Tables for Stemming and Lemmatization.},
	author = {Nicolai, Garrett  and Kondrak, Grzegorz},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	month = {aug},
	year = {2016},
	address = {Berlin, Germany},
	publisher = {Association for Computational Linguistics},
	url = {https://www.aclweb.org/anthology/P16-1108},
	doi = {10.18653/v1/P16-1108},
	pages = {1138--1147},
}

@INPROCEEDINGS{bigData,
	author={M. U. {Çakir} and S. {Güldamlasioğlu}},
	booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)},
	title={Text Mining Analysis in Turkish Language Using Big Data Tools},
	year={2016},
	volume={1},
	number={},
	pages={614-618},
	keywords={Big Data;data analysis;data mining;feature selection;natural language processing;pattern clustering;social networking (online);text analysis;text mining analysis;Turkish language;Big Data infrastructure;social media;feature selection;topic clustering;Sparks;Text mining;Media;Big data;Data models;Feature extraction;Clustering algorithms;Text mining;Text preprocessing;Topic modeling;Text clustering;Big data},
	doi={10.1109/COMPSAC.2016.203},
	ISSN={},
	month={June},}

@article{pcaObj,
	title={Principal component analysis},
	author={Abdi, Herv{\'e} and Williams, Lynne J},
	journal={Wiley interdisciplinary reviews: computational statistics},
	volume={2},
	number={4},
	pages={433--459},
	year={2010},
	publisher={Wiley Online Library}
}

@book{truncatedSVD,
	author = {Hauck, Trent},
	title = {Scikit-Learn Cookbook},
	year = {2014},
	isbn = {1783989483, 9781783989485},
	publisher = {Packt Publishing},
} 

@incollection{nmfBook,
	title = {Chapter 5 - Nonnegative Matrix Factorization},
	editor = {Jen-Tzung Chien},
	booktitle = {Source Separation and Machine Learning},
	publisher = {Academic Press},
	pages = {161 - 229},
	year = {2019},
	isbn = {978-0-12-817796-9},
	doi = {https://doi.org/10.1016/B978-0-12-804566-4.00017-6},
	url = {http://www.sciencedirect.com/science/article/pii/B9780128045664000176},
	author = {Jen-Tzung Chien},
	keywords = {signal-channel source separation, probabilistic nonnegative factorization, variational Bayesian, sampling method, Bayesian learning, sparse learning, discriminative learning, deep learning, speech dereverberation, speech separation, music signal separation, singing voice separation, music information retrieval}
}

@INPROCEEDINGS{tfidfDrawback,
	author={S. {Qu} and S. {Wang} and Y. {Zou}},
	booktitle={2008 International Seminar on Future Information Technology and Management Engineering},
	title={Improvement of Text Feature Selection Method Based on TFIDF},
	year={2008},
	volume={},
	number={},
	pages={79-81},
	keywords={pattern classification;text analysis;vectors;text feature selection method;TFIDF;text classification method;distance vector;Text categorization;Vocabulary;Frequency;Information management;Aggregates;Communications technology;Seminars;Information technology;Technology management;Engineering management},
	doi={10.1109/FITME.2008.25},
	ISSN={},
	month={Nov},}

@article{knnDrawback,
	title={Efficient k-nearest neighbor join algorithms for high dimensional sparse data},
	author={Wang, Jijie and Lin, Lei and Huang, Ting and Wang, Jingjing and He, Zengyou},
	journal={arXiv preprint arXiv:1011.2807},
	year={2010}
}

@INPROCEEDINGS{sparsePCA-get-a-read-on,
	author={F. R. {On} and R. {Jailani} and S. L. {Hassan} and N. M. {Tahir}},
	booktitle={2016 IEEE 12th International Colloquium on Signal Processing Its Applications (CSPA)},
	title={Analysis of sparse PCA using high dimensional data},
	year={2016},
	volume={},
	number={},
	pages={340-345},
	keywords={data analysis;feature extraction;learning (artificial intelligence);principal component analysis;sparse PCA analysis;high dimensional data;sparse principal component analysis;feature extraction;UCI machine learning high dimensionality data;PCA technique;Principal component analysis;Data models;Feature extraction;Biological system modeling;Sparse matrices;Artificial neural networks;Loading;Principal component analysis (PCA);Sparse principal component analysis (Sparse PCA);High dimensional data;feature extraction;classifier},
	doi={10.1109/CSPA.2016.7515857},
	ISSN={},
	month={March},}

@Article{WONGSO2017137,
  author   = {Rini Wongso and Ferdinand Ariandy Luwinda and Brandon Christian Trisnajaya and Olivia Rusli and Rudy},
  title    = {News Article Text Classification in Indonesian Language},
  journal  = {Procedia Computer Science},
  year     = {2017},
  volume   = {116},
  pages    = {137 - 143},
  issn     = {1877-0509},
  note     = {Discovery and innovation of computer science technology in artificial intelligence era: The 2nd International Conference on Computer Science and Computational Intelligence (ICCSCI 2017)},
  abstract = {This research intends to find the appropriate algorithm to automatically classify a news article in Indonesian Language. We obtain our dataset which is taken by using a web crawling method from www.cnnindonesia.com. First of all, the document will first undergo some Text Preprocessing method in the form of Lemmatization and Stopwords Removal. The reason we are doing the Text Preprocessing step before anything else is to minimize the noise in the document. Next, we apply Feature Selection onto the document to further separate important words and less important words inside the document. After applying Feature Selection, the document will be classified by the classifier. We are comparing the TF-IDF and SVD algorithm for feature selection, while also comparing the Multinomial Naïve Bayes, Multivariate Bernoulli Naïve Bayes, and Support Vector Machine for the Classifiers. Based on the test results, the combination of TF-IDF and Multinomial Naïve Bayes Classifier gives the highest result compared to the other algorithms, which precision is 0.9841519 and its recall is 0.9840000. The result outperform the previous similar study that classify news article in Indonesian language which obtained 85% of accuracy.},
  doi      = {https://doi.org/10.1016/j.procs.2017.10.039},
  keywords = {Classification, , TF-IDF, Multinomial Naïve Bayes},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050917320872},
}

@article{arabicNews,
	title = "SANAD: Single-label Arabic News Articles Dataset for automatic text categorization",
	journal = "Data in Brief",
	volume = "25",
	pages = "104076",
	year = "2019",
	issn = "2352-3409",
	doi = "https://doi.org/10.1016/j.dib.2019.104076",
	url = "http://www.sciencedirect.com/science/article/pii/S2352340919304305",
	author = "Omar Einea and Ashraf Elnagar and Ridhwan Al Debsi",
	keywords = "Arabic, Natural language processing, News articles, Single-label text classification",
	abstract = "Text Classification is one of the most popular Natural Language Processing (NLP) tasks. Text classification (aka categorization) is an active research topic in recent years. However, much less attention was directed towards this task in Arabic, due to the lack of rich representative resources for training an Arabic text classifier. Therefore, we introduce a large Single-labeled Arabic News Articles Dataset (SANAD) of textual data collected from three news portals. The dataset is a large one consisting of almost 200k articles distributed into seven categories that we offer to the research community on Arabic computational linguistics. We anticipate that this rich dataset would make a great aid for a variety of NLP tasks on Modern Standard Arabic (MSA) textual data, especially for single label text classification purposes. We present the data in raw form. SANAD is composed of three main datasets scraped from three news portals, which are AlKhaleej, AlArabiya, and Akhbarona. SANAD is made public and freely available at https://data.mendeley.com/datasets/57zpx667y9."
}

@Comment{jabref-meta: databaseType:bibtex;}

@COMMENT
End here
