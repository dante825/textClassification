%!TEX root=thesis.tex
%This is the draft abstract. More soon, I promise!
Text classification is one of the main tasks in \ac{nlp}. Text classification assigns text into pre-defined categories. Text classification is needed in news articles to distinct the articles into different categories for ease of searching and viewing. Some news articles in different categories has quite similar features which is a challenge to text classification method. The common document representation method used to extract features from text usually produce a large and sparse matrix. The high dimensionality of the matrix would be a hindrance to the accuracy of the classification models. Thus, dimension reduction algorithm is applied to reduce the dimension of these large and sparse matrix. This study analyse effect of dimension reduction has on different document representation algorithms and subsequently the performance of the classification models. It is found that \ac{svm} has the best overall performance compare to \ac{knn} and \ac{nn}. Based on the result of this research, \ac{tfidf} is the better document representation method compare to term frequency. The performance of classification models decreased when dimension reduction is applied since the number of features available is lesser. Dimension reduction is a computationally expensive process thus it increase the time taken. The best result is achieved without dimension reduction but dimension reduction still would serve its purpose when there is memory constraint to store the features. \\

\textbf{Keywords: } text classification, dimension reduction, \ac{tfidf}, \ac{knn}, \ac{svm}, \ac{nn}.
