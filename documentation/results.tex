%!TEX ROOT = main.tex

\chapter{Results and Discussions}

\section{Introduction}
The results of the experiments are shown in this section. The differences in the accuracy and performance of each of the methods would be discussed and analysed. Hopefully, the results and discussions would be able to answer the research questions posted in the beginning of this research.

\section{Term frequency}

\begin{table}[ht]
	\centering
	\begin{tabular}{|| c | c | c | c||}
		\hline
		ML & no of features & accuracy & time taken (s) \\ [0.5ex]
		\hline\hline
		kNN & 8000 & 0.31 & 3.74 \\ 
		\hline
		SVM & 8000 & 0.81 & 5.27 \\
		\hline
		NN & 8000 & 0.84 & 91.82 \\
		\hline
	\end{tabular}
\caption{Term frequency}
\label{tbl:termFrequency}
\end{table}

Term frequency is one of most common feature extraction method in text. It convert all the words in the dataset into a matrix where each column is a word and the value in each of the cells are the number of times each word appear in the text. Each row in the matrix is a document. This vector space model representation of the words would result in a sparse matrix since each of the documents only contains a subset of all the words in the whole dataset. 

In this experiment, the number of features of vector space model from term frequency extraction is limited to 8000, this is because of the memory constraint of the machine, if it is unlimited, the resulting matrix would be of bigger size and would have the probability of running out of memory while processing the matrix. 

In the results shown above, \ac{nn} achieved an accuracy of 0.84 which is the best accuracy out of the 3 classification algorithm but it is also the one that takes the longest to process which is 91.82s. \Ac{knn} took the least time to process, 3.74s but has the lowest accuracy score, 0.31. Overall performance, \ac{svm} is the best, it only took 5s to process, which is 80s less than \ac{nn} and has an accuracy of 0.81 which is comparable with \ac{nn}. 

\Ac{knn} accuracy is low in this scenario is possibly due to the sparsity of the feature matrix or vector space model. The data points are few and far in between. \Ac{knn} is designed for low dimensional data therefore it doesn't perform well in large and sparse data. The underlying reason would be due to \ac{knn} classify a new data point based on the distance of the new data point with the nearby data points. Since the other data points are few and sparse, the classification of the new data points would be biased to the few data points that are near. This bias would reduce the accuracy of the classification. \cite{knnDrawback}

Neural Network (\ac{nn}) took the longest time to process the features, this is due to the layers of neurons in \ac{nn}. The vector space model even though sparse has high dimension, \ac{nn} would need as much if not more neurons as the number of features to process the data. This processing would takes both time and computing power.

\Ac{svm} depends on the prominent features of the dataset, the support vectors to classify the dataset so it is relatively faster and as accurate.

\clearpage
\section{Term frequency with naive dimension reduction}

\begin{table} [ht]
	\centering
	\begin{tabular}{|| c | c | c | c | c||}
		\hline
		ML & parameter value & no of features & accuracy & time taken (s) \\ [0.5ex]
		\hline\hline
		kNN & 100 & 2530 & 0.34 & 3.94 \\ 
		\hline
		kNN & 500 & 478 & 0.42 & 4.66 \\ 
		\hline
		kNN & 1000 & 173 & 0.41 & 5.17 \\ 
		\hline\hline
		SVM & 10 & 12705 & 0.83 & 6.10 \\
		\hline
		SVM & 100 & 2530 & 0.76 & 5.67 \\
		\hline
		SVM & 500 & 478 & 0.63 & 20.20 \\
		\hline\hline
		NN & 10 & 12705 & 0.87 & 116.78 \\
		\hline
		NN & 100 & 2530 & 0.80 & 48.08 \\
		\hline
		NN & 500 & 478 & 0.65 & 61.87 \\
		\hline
	\end{tabular}
\caption{Term frequency with naive dimension reduction}
\label{tbl:termFrequencyNaive}
\end{table}

The feature extraction method used in this experiment is same as the experiment above but dimension reduction method is applied to the resulting matrix before training. The dimension reduction algorithm used in this experiment is a naive one which means that the columns with the least occurrence of word are removed assuming those words are unimportant and would not have much influence on the accuracy.

The parameter value is just an integer value passed to the dimension reduction function implemented. It has an inversely proportional relationship with the number of features. The larger the parameter value, the more columns would be removed and the number of features would decrease.

In this experiment, \ac{knn} still has the worst performance among the 3 which is at 0.34 with 2530 number of features. As the number of features decreases to 173, the accuracy increases slightly to 0.41. As the number of features decreases, the time taken for \ac{knn} to produce the result increases from 3.946s to 5.17s.

\Ac{svm} and \ac{nn} have the same trend in this experiment, the accuracy decreases as the number of features decreases. Accuracy of \ac{svm} decreases from 0.83 to 0.63 as the number of features decreases from 12705 to 478. The time taken by \ac{svm} fluctuates as the amount of features decreases, it decrease from 6s to 5s when the amount of features decreases from 12705 to 2530, but the time taken increases from 5s to 20s, as the number of features decreases further to 478. The decrement in accuracy in \ac{svm} is slight in relative to the decrement in the number of features. Accuracy decreased by 0.20 while the number features has decreased by 12227.

The time taken by \ac{svm} increases quite drastically when the number of features decreased. The sparsity of the vector space model decreases, resulting in a denser vector space model. \Ac{svm} need to take into account more data points to calculate an effective hyperplane to separate the data points into different classes.

On the other hand, the accuracy of \ac{nn} decreases from 0.87 to 0.65 as the number of features decreases from 12705 to 478. However, the time taken by NN fluctuates from 116.78s to 48.08s and then to 61.87s when the amount of features decreases from 12705 to 2530 then to 478. The overall decrement in time taken is possibly due to the reduction of features, less neurons are needed to process the data and thus the speedup.

In the scenarios where \ac{svm} and \ac{nn} have more features than the first experiment with just term frequency, both of the classification algorithms have better accuracy. These 2 classification algorithms are efficient and able to process features in a large and sparse vector space.


\clearpage
\section{Term frequency with truncated SVD}

\begin{table} [ht]
	\centering
	\begin{tabular}{|| c | c | c | c||}
		\hline
		ML & no of features & accuracy & time taken (s) \\ [0.5ex]
		\hline\hline
		kNN & 4000 & 0.31 & 482.66 \\
		\hline
		kNN & 2000 & 0.38 & 216.59 \\ 
		\hline
		kNN & 500 & 0.49 & 60.89 \\ 
		\hline
		kNN & 100 & 0.55 & 15.38 \\ 
		\hline
		kNN & 10 & 0.30 & 2.78 \\ 
		\hline\hline
		SVM & 4000 & 0.80 & 370.43 \\
		\hline
		SVM & 2000 & 0.78 & 172.58 \\
		\hline
		SVM & 500 & 0.77 & 85.03 \\
		\hline\hline
		NN & 4000 & 0.80 & 220.76 \\
		\hline
		NN & 2000 & 0.79 & 91.13 \\
		\hline
		NN & 500 & 0.77 & 40.86 \\
		\hline
	\end{tabular}
\caption{Term frequency with SVD}
\label{tbl:termFrequencySvd}
\end{table}

In this experiment, the feature extraction or document representation method used is still term frequency but the dimension reduction algorithm has been changed. Instead of reducing the dimension of the feature matrix naively by removing the terms that has the lowest frequency, truncated \ac{svd} is used. Truncated \ac{svd} would reduce the dimension of the vector space model by retrieving the features with maximum variance in the data.

The number of features shown in the table above is the number of columns in the resulting matrix after truncated \ac{svd} is applied.

Similar with the trend in the previous experiment (term frequency with naive dimension reduction), the accuracy of \ac{knn} increases slightly, from 0.31 to 0.55 when the features decreases from 4000 to 100, but the resulting accuracy is still far from satisfactory. The time taken by \ac{knn} decreases from 482s to 15s as the number of features decreases. The trend of accuracy increment as the number of features decrease cease when the number of features is reduced to 10. \Ac{knn} accuracy decrease from 0.55 to 0.30 at that point. The fluctuation of the accuracy would be due to \ac{knn} dependency on Euclidean distance between the data points to classify a new data. As the amount of features decrease, the feature matrix becomes less sparse, \ac{knn} would need to process less data points thus the speedup. A less sparse matrix resulting from the decrement of features also make \ac{knn} classification more accurate and less biased but when the features decrease to an extent where there is no sufficient data to classify a data point correctly, the accuracy dropped.

The accuracy of \ac{svm} and \ac{nn} also have the same trend with the previous experiment, the accuracy decreases slightly when the number of features decreases. Accuracy of \ac{svm} decreases from 0.80 to 0.78 then to 0.77 and the accuracy of \ac{nn} decreases from 0.80 to 0.79 then further to 0.77 as the number of features decreases from 4000 to 2000 and 500.

As expected, the time taken by the classification model to reduce the dimension and predict the result decreases as the number of features decreases. However, when compare with previous experiments, the time taken in this experiment is astoundingly higher in the case where the features amount to 4000 and 2000. \Ac{svd} would be the culprit, dimension reduction comes at a cost which is processing power. To calculate the maximum variance of the features and transform the feature matrix into a smaller dimension would require no small feats of calculation, this would consume both time and processing power.

However, the resulting accuracy is not as good as just with term frequency. This is expected because the number of features decreased, the classification would have lesser information to classify a new data point correctly. The reduction in features may save some memory space but in order to achieve that more processing power and time would be needed.

Comparing the performance of the classification models in this experiment with the second experiment, term frequency with naive dimension reduction, the performance of the classification models are almost similar when the amount of features are around 2000. In the scenarios where the amount of features is around 500, \ac{svd} has a better performance. The classification models with \ac{svd} achieved a higher accuracy than that of naive dimension reduction. At this stage, the advantage of \ac{svd} over naive dimension reduction is shown. \Ac{svd} transformed the feature matrix into smaller dimension but retaining the maximum variance or the most prominent feature of the data. Therefore, models trained with \ac{svd} should have a better performance than those that trained with naive dimension reduction.

The next experiments would apply different document representation algorithm to investigate further the effect of dimension reduction has on classification model performance and compare the performance of different document representation method.\\

\clearpage
\section{TF-IDF}

\begin{table} [ht]
	\centering
	\begin{tabular}{|| c | c | c | c||}
		\hline
		ML & no of features & accuracy & time taken (s) \\ [0.5ex]
		\hline\hline
		kNN & 8000 & 0.76 & 3.71 \\ 
		\hline
		SVM & 8000 & 0.87 & 2.39 \\
		\hline
		NN & 8000 & 0.88 & 55.82 \\
		\hline
	\end{tabular}
\caption{TF-IDF}
\label{tbl:tfidf}
\end{table}

The previous experiments found that \ac{svd} has a slight edge over naive dimension reduction when the number of features decreased to a certain extend. In this and the next experiments, the effect on dimension reduction is further explored. A different document representation algorithm is applied in this and the next 2 experiments.
 
The document representation algorithm applied is \ac{tfidf}. In contrast with term frequency which only take the frequency of each word into account, \ac{tfidf} takes both the frequency of each word and the rarity of it into account. If a term or word appear in high frequency but in many documents, this word may not be of importance and consequently is not a meaningful feature. If a word appear rarely and only in a few documents, this word would have high importance and would be meaningful feature of the few documents.

With \ac{tfidf}, \ac{knn} can achieved a satisfactory accuracy score of 0.76 even though the number of features in the resulting matrix of \ac{tfidf} is the same with term frequency which is 8000. The vector space model of \ac{tfidf} would not be as sparse as that of term frequency which is \ac{knn} is more suitable for \ac{knn}.

\Ac{nn} is still provide the highest accuracy score of 0.88 but the time taken also the longest at 55.82s.

\Ac{svm} achieved an accuracy of 0.87 which is just 0.01 shy of what achieved by \ac{nn} and the time taken is the lowest among the 3 which is 2.39s. \Ac{svm} can achieve high accuracy even with high dimension data, because \ac{svm} uses the prominent features or support vectors from the data to perform classification, \ac{svm}'s computational complexity is independent of the dimension of the data. \cite{dimRedCat}

In comparison with the first experiment that apply term frequency document representation without dimension reduction, the performance of the classification models significantly improve. \Ac{knn} accuracy has more than doubled from 0.31 to 0.76. \Ac{svm} accuracy increases from 0.81 to 0.87 while accuracy of \ac{nn} increases from  0.84 to 0.88. Besides accuracy, the time taken also improved, time taken by \ac{svm} reduces from 5.27s to 2.39s while time taken by \ac{nn} reduces from 91.82s to 55.82s. All these improvements are achieved with the same number of features in the vector space, which is 8000. 

The performance of the classification models in this experiment is also better than those in the 2 experiments above with dimension reduction. However this may not be a fair comparison because the different number of features are used and dimension reductions are not applied in this experiment. Dimension reduction algorithms would be applied to the vector space model from \ac{tfidf} in the following experiments in order to have a fair comparison.

The performance increment from term frequency to \ac{tfidf} seems to prove that \ac{tfidf} is a better document representation method.\\

\clearpage
\section{TF-IDF with naive dimension reduction}

\begin{table} [ht]
	\centering
	\begin{tabular}{|| c | c | c | c | c||}
		\hline
		ML & parameter value & no of features & accuracy & time taken (s) \\ [0.5ex]
		\hline\hline
		kNN & 50 & 4221 & 0.74 & 4.00 \\ 
		\hline
		kNN & 100 & 2530 & 0.71 & 4.11 \\ 
		\hline
		kNN & 500 & 478 & 0.49 & 5.15 \\ 
		\hline\hline
		SVM & 50 & 4221 & 0.86 & 2.58 \\
		\hline
		SVM & 100 & 2503 & 0.83 & 2.63 \\
		\hline
		SVM & 500 & 478 & 0.66 & 2.94 \\
		\hline\hline
		NN & 50 & 4221 & 0.85 & 38.80 \\
		\hline
		NN & 100 & 2530 & 0.83 & 34.28 \\
		\hline
		NN & 500 & 478 & 0.64 & 77.12 \\
		\hline
	\end{tabular}
\caption{TF-IDF with naive dimension reduction}
\label{tbl:tfidfNaive}
\end{table}

Similar with the experiment with term frequency, naive dimension reduction is applied to the vector space model generated from \ac{tfidf}. The trend over all the 3 machine learning models when the number of features decreases are similar. The accuracy of the classification models decreases and the time taken increases. 

The accuracy achieved by \ac{knn} with \ac{tfidf} plus naive dimension reduction is still passable at 0.74 when the features reduced from 8000 to 4221. \Ac{knn}'s accuracy dropped slightly to 0.71 when the number of features decreases to 2530. When the number of features decreases from 2530 to 478, as expectedly the accuracy of \ac{knn} dropped for quite a large margin, from 0.71 to 0.49. The time taken by \ac{knn} increases slightly from 4s to 5s as the features decreases.

\Ac{svm} has the same behaviour with \ac{knn} in this experiment, its accuracy decreases from 0.86 to 0.83 and then to 0.66 when the number of features decreases from 4221 to 2503 to 478. The time taken increases slightly as well as the number of features decreases.

\Ac{nn} also has the similar trend in accuracy as the number of features decreases. \Ac{nn}'s accuracy decreases from 0.85 to 0.64 when the number of features decreases from 4221 to 478. The differences in \ac{nn} is that the time taken almost doubled when the number of features is low compare to when the number of features is high. The time taken is 38s when there is 4221 features and when there is only 478 features, the time taken doubled to 77s.

In the scenarios where the number of features are almost halved, from 8000 to 4221, the performance of the classification models are still comparable with the performance achieved with just \ac{tfidf} without any dimension reduction. The accuracy are almost the same, the time taken is similar except \ac{nn} which has quite a speedup when the number of features is reduced to 4221.

It can be deduced that with naive dimension reduction, the number of features and memory needed to store the vector space model is reduced. With this reduced number of features, the classification models can still achieve comparable performance at a slightly decreased capacity.

This is mainly because of the reduction in features. As the amount of information became lesser, less information is available to train a comprehensive model. Therefore, the accuracy of the models decrease. Even though the dimension reduction applied is a naive one and less computing intensive, it still increases the time taken compared with just with \ac{tfidf}. The more reduction is performed, the time taken would increase as well.

\clearpage
\section{TF-IDF with truncated SVD}

\begin{table} [ht]
	\centering
	\begin{tabular}{|| c | c | c | c||}
		\hline
		ML & no of features & accuracy & time taken (s) \\ [0.5ex]
		\hline\hline
		kNN & 4000 & 0.77 & 457.16 \\
		\hline
		kNN & 2000 & 0.58 & 208.64 \\ 
		\hline
		kNN & 500 & 0.55 & 58.58 \\ 
		\hline
		kNN & 100 & 0.69 & 15.09 \\
		\hline
		kNN & 50 & 0.69 & 7.58 \\ 
		\hline
		kNN & 10 & 0.55 & 2.54 \\ 
		\hline\hline
		SVM & 4000 & 0.87 & 189.32 \\
		\hline
		SVM & 2000 & 0.86 & 69.63 \\
		\hline
		SVM & 500 & 0.83 & 17.54 \\
		\hline\hline
		NN & 4000 & 0.85 & 215.73 \\
		\hline
		NN & 2000 & 0.84 & 87.12 \\
		\hline
		NN & 500 & 0.81 & 39.93 \\
		\hline
	\end{tabular}
\caption{TF-IDF with SVD}
\label{tbl:tfidfSvd}
\end{table}

In this last experiment, truncated \ac{svd} dimension reduction is applied to the resulting vector space model from \ac{tfidf}. Similar with experiment before, each of the classification models would be tested with several set of vector space model, each with different number of features. To put it in perspective, the number of features without reduction is 8000. 

When the number of features are at 4000 which is halved, \ac{knn} still can produced an accuracy of 0.77 which is similar with what is achieved with \ac{tfidf} without dimension reduction. Same goes to \ac{svm} and \ac{nn}, at 4000 features, the accuracy are quite similar with \ac{tfidf} without dimension reduction. However, when the dimension of the vector space model is reduced to 2000, the accuracy across the 3 classification models dropped. \Ac{knn} being the most drastic, its accuracy dropped to 0.58 while \ac{svm} and \ac{nn} dropped to 0.86 and 0.84 respectively, which is slightly worse than before but it is still satisfactory.

Comparing with the results of the experiment with \ac{tfidf} and naive dimension reduction, this performance of the classification models with truncated \ac{svd} has a slight advantage. The accuracy is slightly better with truncated \ac{svd} than that with naive dimension reduction. This would be due to the advantage of truncated \ac{svd} obtaining the maximum variance of the features over naive dimension reduction.

The time taken in this experiment is much higher than the experiment of \ac{tfidf} without dimension reduction and \ac{tfidf} with naive dimension reduction which is expected. This would be due to \ac{svd} reduction takes more time as more calculation is needed to transform the data. However, the time taken decreased when the further reduction is done, \ac{knn} take 457s to reduce the vector space model from 8000 to 4000 but just 2s to reduce the vector space model from 8000 to 10. Keep in mind that the time recorded here includes the time taken for the classification model to predict the test dataset as well as the dimension reduction time. This trend appear in \ac{svm} and \ac{nn} as well. \Ac{svm} took 189s to reduce 8000 to 4000 but just 18s to reduce 8000 to 500 while \ac{nn} took 215s to reduce 8000 to 4000 and 40s to reduce 8000 to 500. 

The decrement in time could be explained by the reduction of features, as the amount of features decreases, the time taken to process them also reduced. Therefore there is a speedup. This speedup that comes with the reduction of features, comes at a cost which is accuracy. For \ac{svm} and \ac{nn}, the reduction in accuracy is meagre thus it would be logical to trade the slight accuracy with the speedup but in \ac{knn}, the trade off would be lopsided in the favour of time taken.\\

\clearpage
\section{Accuracy VS dimension of term frequency matrix}
\begin{figure} [ht]
	\centering
	\includegraphics[width=\textwidth]{tfsvd}
	\caption{The effect of dimension reduction on accuracy with term frequency matrix}
	\label{fig:freqCount}
\end{figure}

This figure illustrates the effect of dimension reduction has on the accuracy of the classification models. The feature extraction algorithm used here is term frequency and the dimension reduction algorithm used is truncated \ac{svd}.

\Ac{knn} accuracy increases from 0.3 to 0.55 as the dimension of the feature matrix decreases from 8000 to 100. After that point, further decrease in dimension does not increase the accuracy of \ac{knn} further but decrease its accuracy back to 0.3.

\Ac{svm} and \ac{nn} have the similar trend as the dimension of the feature matrix reduces. Both of them were at high accuracy, around 0.8 when the dimension of the matrix is at 8000. Both \ac{svm} and \ac{nn} accuracy decrease slightly as the dimension decreases. When the dimension of the matrix is reduced to an extreme, 10, both \ac{svm} and \ac{nn} accuracy dropped to around 0.3.

This showed that \ac{knn} is a classification model that is sensitive to dimension changes and is more efficient at lower dimension. \ac{svm} and \ac{nn}, however, are more resilient to the feature matrix dimension changes. Both of \ac{svm} and \ac{nn} accuracy remain stable as the dimension decreases.\\

\clearpage
\section{Accuracy VS dimension of TF-IDF matrix}
\begin{figure} [ht]
	\centering
	\includegraphics[width=\textwidth]{tfidfsvd}
	\caption{The effect of dimension reduction on accuracy with TF-IDF matrix}
	\label{fig:freqCount}
\end{figure}

This figure show the fluctuations of the accuracy of the classification models as the dimension decreases. The feature extraction method used here is \ac{tfidf} and the dimension reduction algorithm used is truncated \ac{svd}.

\Ac{knn} accuracy remain stable at around 0.77 as the dimension decrease from 8000 to 4000. After 4000, further decrement in dimension resulting in a decrease in accuracy. The accuracy of \ac{knn} deflected and increase from 0.55 to 0.69 when the dimension reduce from 500 to 10.

Similar to previous result, \ac{svm} and \ac{nn} accuracy remain stable around 0.8 as the dimension reduces from 8000 to 500. When the dimension of the feature matrix is reduced to an extreme of 10 then the accuracy of both \ac{svm} and \ac{nn} deteriorated to around 0.5.

\Ac{svm} is the best classification model out of the 3, it has slightly better performance than \ac{nn} in most of the scenarios. It is most resilient to the changes in dimension. This is because \ac{svm} depend on the prominent features of the data to classify the data into different classes, the dimension of the data do not has much influence on \ac{svm} computational complexity. \cite{dimRedCat}. \Ac{nn} has comparable accuracy with \ac{svm} but it would take longer time than \ac{svm} to produce the similar result.

\clearpage
\section{Conclusion}
From the results of the 6 experiments above, it is found that \ac{tfidf} is a better document representation algorithm than term frequency. The resulting vector space model from \ac{tfidf} can achieve a higher accuracy than that of term frequency. 

The effect of dimension reduction on the accuracy of the classification models is analysed. Dimension reduction, naive and truncated \ac{svd}, do reduce the dimension of the vector space model, reducing the memory needed to store the matrix. However, this reduction in features and information would result in a loss of accuracy. truncated \ac{svd} would be the better dimension reduction algorithm compared to the naive method because the accuracy achieved with truncated \ac{svd} is higher than that of the naive method.

Among the 3 classification models tested in the experiments, \ac{svm} is the most efficient and versatile. \Ac{svm} can achieve high accuracy (> 0.80) in most scenarios. \Ac{nn} can also achieve high accuracy in most of the cases tested but \ac{nn} is more time consuming. \Ac{svm} has an advantage over \ac{nn} on the aspect of processing time. Therefore, \ac{svm} would be the most efficient text classification model among the 3 classification models.

